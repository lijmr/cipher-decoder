# Cipher Decoder
This is the third project that I completed as part of the Udemy NLP course by Lazy Prorgrammer Inc. that I have been taking over winter break. The link to this course can be found here: https://www.udemy.com/course/data-science-natural-language-processing-in-python/.

The cipher decoder project's primary focus was to practice my understanding of ciphers/coded messages, probabilisitc language models, and genetic algorithms. It uses text from Moby Dick to train the model used to help with decoding. Here are the following steps that I took to complete this project.
  1. Generate a random substitution cipher to be used as the true mapping that we want our genetic algorithm to find. This can be simply done by using ascii_lowercase from the string module as well as the random module.
  2. Create a character-level language model using Markov models. In the current implementation, only first-order Markov models are used. This means that unigrams and bigrams should be used to store the probability of each letter in a word. Read in moby_dick.txt, going word by word to fill in the unigrams and bigrams. Be sure to normalize to ensure accurate probabilities. Additionally, create a log-likelihood function that calculates the fitness of a decoded message using this language model.
  3. Create functions that encode and decode a given message. For the encoding function, be sure to use the true mapping for the cipher. For the decoding function, include a parameter that takes in a cipher to decode the message. Encode a given message so that it can be used as the test for the genetic algorithm.
  4. Run a genetic algorithm on the encoded message to find the true mapping. The genetic algorithm starts off with a pool of random ciphers, generates 3 mutated offspring per parent cipher, then uses a survival-of-the-fittest technique by choosing the 5 ciphers with the greatest log-likelihood. These ciphers are kept to be used in the next iteration in a loop, where mutated offspring are created from these ciphers to continue finding ciphers that maximize the log-likelihood.
  5. Use the cipher that has the highest log-likelihood to decode the message. Compare the resulting decoded message with the original message.

This was definitely the hardest project that I have worked on while going through this course. At first, I had implemented everything to the best of my ability, but when testing my decoder, the resulting decoded message was completely wrong. I eventually decided to follow the code that was given in the lecture. While going through the lecture code, I realized that the biggest bug that caused my decoder to not work was the inclusion of punctuation. More specifically, whenever I would make my language model and encoder include rather than omit punctuation, the log-likelihood scores would be consistently lower than if punctuation was omitted. Unfortunately, I was not able to discover this bug until I after I followed the code given in the lecture, so all my original code is lost (a lesson learned in being better about using git). I suspect that the reason why this bug occurred is because of some unseen issues in traversing the letters of each word; another theory that I have is that when each punctuation mark is replaced by a space, the log-likelihood of an entire message changes to accommodate for a new unigram.

Some ways that I want to continue improving this cipher decoder is to implement trigrams, which allows for a more powerful language model that uses second-order Markov models. Additionally, I would like to figure out how to fix the punctuation issue that I had earlier - it would be cool to see the encoded/decoded messages would maintain their original punctuation which also allows better readability too.

Overall, I had a lot of fun working on this project. Very challenging, but I was able to learn a lot about how genetic algorithms work and how powerful they are in NLP. I'm interested to see how genetic algorithms are used in other areas of machine learning as well.
